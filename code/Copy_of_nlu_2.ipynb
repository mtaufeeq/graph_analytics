{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cbSg2DQPSjj"
   },
   "source": [
    "#### Copyright 2018 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OvPpnyk0PVpf"
   },
   "outputs": [],
   "source": [
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRYekDVFQ4Jr"
   },
   "source": [
    "# Natural Language Understanding: Word Embeddings\n",
    "\n",
    "Please **make a copy** of this Colab notebook before starting this lab. To do so, choose **File**->**Save a copy in Drive**.\n",
    "\n",
    "## Lab outline\n",
    "  1. Creating the vocabulary\n",
    "  1. The co-occurrence matrix\n",
    "  1. Constructing a sparse counts matrix\n",
    "  1. Toy corpus example\n",
    "  1. Positive pointwise mutual information\n",
    "  1. Computing word vectors with SVD\n",
    "  1. Visualization\n",
    "  1. t-SNE for better visualizations\n",
    "  1. Takeaways\n",
    "  1. Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEp3r-VNIm0J"
   },
   "source": [
    "Instead of using Wordnet, let's investigate more data-driven approaches to creating representations of meaning for words. John Rupert Firth, a 20th century linguist, inspired this line of work when he said \"You shall know a word by the company it keeps.\" That is, let's try to infer something about semantics from the distributional statistics of how words are used in context.\n",
    "\n",
    "Early work on [distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics) involved clustering words together with similar distributional statistics. The widely used [Brown clustering algorithm](https://en.wikipedia.org/wiki/Brown_clustering), developed at IBM by Peter Brown in the early 1990s, produces a hierarchical clustering directly from data with some similarity to the Wordnet hierarchy produced by hand.\n",
    "\n",
    "Here, we'll take a step past clusters and start exploring **word embeddings** -- learned mappings of words to continuous vectors. Embedding space allows us to compute the distance between words (in Euclidean space) while preserving the nuance that differentiates synonyms (this nuance is lost in a clustering). Embeddings directly address the sparsity problem with language -- rather than parameterize all of the language with a sparse set of atomic words (or word n-grams), we parameterize it in a single, shared, fairly low dimensional space -- say 100 dimensions. So instead of a million words, we have 100 semantic axes.\n",
    "\n",
    "This notebook starts to address the problem of learning a mapping from words to vectors, an idea that has become the foundation to just about all modern NLU. At a high level, we'll achieve this by estimating vectors for each word that allow us to reconstruct a large matrix of co-occurrence counts across a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N12fKP93-u2u"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Let's get started with the imports needed for the rest of this lab.\n",
    "\n",
    "We'll also need to load some utility functions which will help us work with the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FukClYGiSJHS"
   },
   "source": [
    "### Imports\n",
    "\n",
    "Run this code cell to add in all of the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "98ILZ3Vb5IBe"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import scipy.sparse\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE as tsne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0HkdNBz-zYk"
   },
   "source": [
    "### bAbI task corpus reader\n",
    "\n",
    "Start by running the cell below to load the corpus reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wQNZzQ-p-qne"
   },
   "outputs": [],
   "source": [
    "#@title bAbI Task corpus reader\n",
    "import sys, os\n",
    "import glob\n",
    "import re\n",
    "from collections import namedtuple\n",
    "\n",
    "# Struct types for different lines in the bAbI dataset.\n",
    "# StoryLine represents \"ID text\" lines as (int, string)\n",
    "# QALine represents \"ID question answer support\" lines as\n",
    "# (int, string, string, list(int)).\n",
    "# If tokenized, string fields can be replaced with list(string).\n",
    "StoryLine = namedtuple(\"StoryLine\", [\"id\", \"text\"])\n",
    "QALine = namedtuple(\"QALine\", [\"id\", \"question\", \"answer\", \"support_ids\"])\n",
    "\n",
    "class BabiTaskCorpusReader(object):\n",
    "    \"\"\"Corpus reader for the bAbI tasks dataset.\n",
    "\n",
    "    See https://research.fb.com/downloads/babi/ for details.\n",
    "\n",
    "    This class exposes a similar interface to NLTK's corpus readers, and should\n",
    "    be interchangable with them in many applications.\n",
    "\n",
    "    Example usage:\n",
    "\n",
    "    import babi_utils\n",
    "    import nltk\n",
    "    tok = nltk.tokenize.treebank.TreebankWordTokenizer()\n",
    "    cr = babi_utils.BabiTaskCorpusReader(\"/home/babi/en\",\n",
    "                                         tokenizer=tok.tokenize)\n",
    "    words = list(cr.words())\n",
    "    print words[:8]\n",
    "    # ['John', 'travelled', 'to', 'the', 'hallway', '.', 'Mary', 'journeyed']\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ALL_FILES = [\n",
    "        'qa10_indefinite-knowledge_test.txt',\n",
    "        'qa10_indefinite-knowledge_train.txt',\n",
    "        'qa11_basic-coreference_test.txt',\n",
    "        'qa11_basic-coreference_train.txt',\n",
    "        'qa12_conjunction_test.txt',\n",
    "        'qa12_conjunction_train.txt',\n",
    "        'qa13_compound-coreference_test.txt',\n",
    "        'qa13_compound-coreference_train.txt',\n",
    "        'qa14_time-reasoning_test.txt',\n",
    "        'qa14_time-reasoning_train.txt',\n",
    "        'qa15_basic-deduction_test.txt',\n",
    "        'qa15_basic-deduction_train.txt',\n",
    "        'qa16_basic-induction_test.txt',\n",
    "        'qa16_basic-induction_train.txt',\n",
    "        'qa17_positional-reasoning_test.txt',\n",
    "        'qa17_positional-reasoning_train.txt',\n",
    "        'qa18_size-reasoning_test.txt',\n",
    "        'qa18_size-reasoning_train.txt',\n",
    "        'qa19_path-finding_test.txt',\n",
    "        'qa19_path-finding_train.txt',\n",
    "        'qa1_single-supporting-fact_test.txt',\n",
    "        'qa1_single-supporting-fact_train.txt',\n",
    "        'qa20_agents-motivations_test.txt',\n",
    "        'qa20_agents-motivations_train.txt',\n",
    "        'qa2_two-supporting-facts_test.txt',\n",
    "        'qa2_two-supporting-facts_train.txt',\n",
    "        'qa3_three-supporting-facts_test.txt',\n",
    "        'qa3_three-supporting-facts_train.txt',\n",
    "        'qa4_two-arg-relations_test.txt',\n",
    "        'qa4_two-arg-relations_train.txt',\n",
    "        'qa5_three-arg-relations_test.txt',\n",
    "        'qa5_three-arg-relations_train.txt',\n",
    "        'qa6_yes-no-questions_test.txt',\n",
    "        'qa6_yes-no-questions_train.txt',\n",
    "        'qa7_counting_test.txt',\n",
    "        'qa7_counting_train.txt',\n",
    "        'qa8_lists-sets_test.txt',\n",
    "        'qa8_lists-sets_train.txt',\n",
    "        'qa9_simple-negation_test.txt',\n",
    "        'qa9_simple-negation_train.txt'\n",
    "    ]\n",
    "\n",
    "    def __init__(self, directory, mask=\"qa*.txt\",\n",
    "                 file_list=ALL_FILES,\n",
    "                 file_reader=open,\n",
    "                 tokenizer=lambda s: s.split(),\n",
    "                 verbose=False):\n",
    "        \"\"\"Construct a corpus reader for the bAbI tasks dataset.\n",
    "\n",
    "        Args:\n",
    "            directory: (string) path to bAbI text files (e.g. /home/babi/en/)\n",
    "            mask: (string) file glob to match particular files. Use\n",
    "                \"qa16_*\" e.g. to match task 16.\n",
    "            file_list: (list(string) or None) If None, will glob directory to\n",
    "                find files. Otherwise, will use the given list of basenames.\n",
    "            file_reader: (function string -> fd) optional replacement for\n",
    "                Python's built-in open(...) method, to be used for reading\n",
    "                from alternative file-like objects.\n",
    "            tokenizer: function string -> list(string), used to split\n",
    "                sentences.\n",
    "            verbose: (bool) if true, will print when reading files.\n",
    "        \"\"\"\n",
    "        self._open = file_reader\n",
    "        self._tokenizer = tokenizer\n",
    "        self._verbose = verbose\n",
    "\n",
    "        if file_list:\n",
    "            basenames = glob.fnmatch.filter(file_list, mask)\n",
    "            filenames = [os.path.join(directory, f) for f in basenames]\n",
    "        else:\n",
    "            # Glob directory\n",
    "            pattern = os.path.join(directory, mask)\n",
    "            filenames = glob.glob(pattern)\n",
    "\n",
    "        # Filenames of form qaXX_task-name_train.txt\n",
    "        # Want to sort by XX as a number\n",
    "        key_fn = lambda f: (int(os.path.basename(f).split(\"_\")[0][2:]), f)\n",
    "        self._filenames = sorted(filenames, key=key_fn)\n",
    "        # Filenames should be nonempty!\n",
    "        assert(self._filenames), \"No files found matching [{:s}]\".format(mask)\n",
    "\n",
    "    def filenames(self):\n",
    "        return self._filenames\n",
    "\n",
    "    def parse_line(self, line):\n",
    "        \"\"\"Parse a single line from the bAbI corpus.\n",
    "\n",
    "        Line is of one of the two forms:\n",
    "        ID text\n",
    "        ID question[tab]answer[tab]supporting fact IDs\n",
    "\n",
    "        See https://research.fb.com/downloads/babi/\n",
    "\n",
    "        Args:\n",
    "            line: (string)\n",
    "\n",
    "        Returns:\n",
    "            (id, text) as (int, string)\n",
    "            OR (id, question, answer, [ids]) as (int, string, string, list(int))\n",
    "        \"\"\"\n",
    "        id_text, rest = line.split(\" \", 1)\n",
    "        id = int(id_text)\n",
    "        if \"\\t\" in rest:\n",
    "            question, answer, s_ids_text = rest.split(\"\\t\")\n",
    "            s_ids = map(int, s_ids_text.split())\n",
    "            return QALine(id, question.strip(), answer.strip(), s_ids)\n",
    "        else:\n",
    "            return StoryLine(id, rest.strip())\n",
    "\n",
    "    def tokenize_parsed_line(self, line):\n",
    "        if isinstance(line, StoryLine):\n",
    "            return StoryLine(line.id, self._tokenizer(line.text))\n",
    "        else:\n",
    "            return QALine(line.id,\n",
    "                          self._tokenizer(line.question),\n",
    "                          self._tokenizer(line.answer),\n",
    "                          line.support_ids)\n",
    "\n",
    "    def _line_iterator(self):\n",
    "        for f in self._filenames:\n",
    "            if self._verbose:\n",
    "                print >> sys.stderr, \"Reading {:s}\".format(os.path.basename(f)),\n",
    "            with self._open(f) as fd:\n",
    "                for line in fd:\n",
    "                    yield line.strip()\n",
    "            if self._verbose:\n",
    "                print >> sys.stderr, \"...done!\"\n",
    "\n",
    "    def examples(self, tokenize=True):\n",
    "        \"\"\"Iterator over complete stories (training examples).\n",
    "\n",
    "        A story spans multiple lines, of the form:\n",
    "\n",
    "        1 text one\n",
    "        2 text two\n",
    "        3 text three\n",
    "        4 question[tab]answer[tab]supporting fact IDs\n",
    "\n",
    "        Args:\n",
    "            tokenize: (bool) If true, will tokenize text fields.\n",
    "\n",
    "        Returns:\n",
    "            iterator yielding list(StoryLine|QALine)\n",
    "              if tokenize=True, then text, question, and answer will be\n",
    "              list(string); otherwise they will be plain strings.\n",
    "        \"\"\"\n",
    "        buffer = []\n",
    "        for line in self._line_iterator():\n",
    "            parsed = self.parse_line(line)\n",
    "            if tokenize:\n",
    "                parsed = self.tokenize_parsed_line(parsed)\n",
    "            # If new story item, flush buffer.\n",
    "            if buffer and parsed.id <= buffer[-1].id:\n",
    "                yield buffer\n",
    "                buffer = []\n",
    "            buffer.append(parsed)\n",
    "        # Flush at end.\n",
    "        yield buffer\n",
    "        buffer = []\n",
    "\n",
    "    def _raw_sents_impl(self, stories=False, questions=False, answers=False):\n",
    "        for line in self._line_iterator():\n",
    "            parsed = self.parse_line(line)\n",
    "            if isinstance(parsed, StoryLine) and stories:\n",
    "                yield parsed.text\n",
    "            else:\n",
    "                if questions:\n",
    "                    yield parsed.question\n",
    "                if answers:\n",
    "                    yield parsed.answer\n",
    "\n",
    "    def raw_sents(self):\n",
    "        \"\"\"Iterator over utterances in the corpus.\n",
    "\n",
    "        Returns untokenized sentences.\n",
    "\n",
    "        Returns:\n",
    "            iterator yielding string\n",
    "        \"\"\"\n",
    "        return self._raw_sents_impl(stories=True,\n",
    "                                    questions=True,\n",
    "                                    answers=True)\n",
    "\n",
    "    def sents(self):\n",
    "        \"\"\"Iterator over utterances in the corpus.\n",
    "\n",
    "        Returns tokenized sentences, a la NLTK.\n",
    "\n",
    "        Returns:\n",
    "            iterator yielding list(string)\n",
    "        \"\"\"\n",
    "        for sentence in self.raw_sents():\n",
    "            yield self._tokenizer(sentence)\n",
    "\n",
    "\n",
    "    def words(self):\n",
    "        \"\"\"Iterator over words in the corpus.\n",
    "\n",
    "        Returns:\n",
    "            iterator yielding string\n",
    "        \"\"\"\n",
    "        for sentence in self.sents():\n",
    "            for word in sentence:\n",
    "                yield word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ScfVx4MTDRjk"
   },
   "source": [
    "### Text corpora\n",
    "\n",
    "We need a corpus of text for our experiments. We'll start with the [bAbI corpus](https://research.fb.com/downloads/babi/), a list of simple sentences (generated from a template) with a very small vocabulary. The number of word **tokens** (strings of characters or punctuation delimited by white space or newline) is over 1 million, but the number of word **types** (the vocabulary) is only 162. This corpus makes it easy to verify that our learned embeddings are reasonable, but it's not representative of the tremendous variation and large vocabularies in natural language.\n",
    "\n",
    "The old standard [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus), by contrast, also contains about 1 million tokens, but over 40,000 types. Incidentally, the Brown corpus is the first major computer-readable linguistic corpus, assembled in 1961 (at Brown University -- no relations to Brown clustering) consisting of American English sampled from 15 different text categories ranging from news text to academic articles to popular fiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GU2C8GVdF6b1",
    "outputId": "7483f2e8-dcee-475a-dc6d-8c7f5cd7c8b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/mledu-datasets/babi_tasks_1-20_v1-2.tar.gz -O /tmp/babi_tasks_1-20_v1-2.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CO3lI3xhVA54"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/babi_tasks_1-20_v1-2.tar.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtarfile\u001b[39;00m\n\u001b[0;32m      4\u001b[0m local_tar \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/tmp/babi_tasks_1-20_v1-2.tar.gz\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m tar_ref \u001b[38;5;241m=\u001b[39m \u001b[43mtarfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_tar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr:gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m tar_ref\u001b[38;5;241m.\u001b[39mextractall(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/tmp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m tar_ref\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rdevs\\lib\\tarfile.py:1638\u001b[0m, in \u001b[0;36mTarFile.open\u001b[1;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[0;32m   1636\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1637\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CompressionError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown compression type \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m comptype)\n\u001b[1;32m-> 1638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(name, filemode, fileobj, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1641\u001b[0m     filemode, comptype \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rdevs\\lib\\tarfile.py:1684\u001b[0m, in \u001b[0;36mTarFile.gzopen\u001b[1;34m(cls, name, mode, fileobj, compresslevel, **kwargs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CompressionError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip module is not available\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1683\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1684\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompresslevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\rdevs\\lib\\gzip.py:173\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m    171\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 173\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmyfileobj \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/babi_tasks_1-20_v1-2.tar.gz'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "\n",
    "local_tar = '/tmp/babi_tasks_1-20_v1-2.tar.gz'\n",
    "tar_ref = tarfile.open(local_tar, 'r:gz')\n",
    "tar_ref.extractall('/tmp')\n",
    "tar_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WmIJ3Xdt5cPX"
   },
   "outputs": [],
   "source": [
    "# print 'Loading bAbI corpus... ',\n",
    "babi_corpus = BabiTaskCorpusReader(\n",
    "    \"/tmp/tasks_1-20_v1-2/en\",\n",
    "    tokenizer=nltk.tokenize.treebank.TreebankWordTokenizer().tokenize)\n",
    "# print 'Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzgcRt9qL-M3"
   },
   "source": [
    "Let's make sure we have the data. The corpus object gives us access to the sentences, via corpus.sents(), which is a list of lists of tokens.\n",
    "\n",
    "(Note: the following code cell may take 20-30 seconds to complete running.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2dt5cEBDWRz",
    "outputId": "fd5abaad-4576-44b1-ea68-c084c4e91421"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/tasks_1-20_v1-2/en\\\\qa1_single-supporting-fact_test.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbabi_corpus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSentences:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(corpus))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Print the first 5 sentences of the corpus.\u001b[39;00m\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mBabiTaskCorpusReader.sents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msents\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;124;03m\"\"\"Iterator over utterances in the corpus.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    Returns tokenized sentences, a la NLTK.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m        iterator yielding list(string)\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_sents():\n\u001b[0;32m    229\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer(sentence)\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mBabiTaskCorpusReader._raw_sents_impl\u001b[1;34m(self, stories, questions, answers)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raw_sents_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, stories\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, questions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, answers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 198\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_line_iterator():\n\u001b[0;32m    199\u001b[0m         parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_line(line)\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(parsed, StoryLine) \u001b[38;5;129;01mand\u001b[39;00m stories:\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mBabiTaskCorpusReader._line_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mprint\u001b[39m \u001b[38;5;241m>>\u001b[39m sys\u001b[38;5;241m.\u001b[39mstderr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading \u001b[39m\u001b[38;5;132;01m{:s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(f)),\n\u001b[1;32m--> 159\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m fd:\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/tasks_1-20_v1-2/en\\\\qa1_single-supporting-fact_test.txt'"
     ]
    }
   ],
   "source": [
    "corpus = list(babi_corpus.sents())\n",
    "print('Sentences:', len(corpus))\n",
    "\n",
    "# Print the first 5 sentences of the corpus.\n",
    "for i, sent in enumerate(corpus[:5]):\n",
    "    print(i, ' '.join(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZkWfXdJEIYD"
   },
   "source": [
    "### Corpus utilities\n",
    "\n",
    "Next, let's load some utility functions which will help us work with the corpus. Run each of the following three cells to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "O_iG4zgkFE6s"
   },
   "outputs": [],
   "source": [
    "#@title Utilities\n",
    "import re\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# For pretty-printing\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "UNK_TOKEN   = u\"<unk>\"\n",
    "\n",
    "def flatten(list_of_lists):\n",
    "    \"\"\"Flatten a list-of-lists into a single list.\"\"\"\n",
    "    return list(itertools.chain.from_iterable(list_of_lists))\n",
    "\n",
    "def pretty_print_matrix(M, rows=None, cols=None, dtype=float, float_fmt=\"{0:.04f}\"):\n",
    "    \"\"\"Pretty-print a matrix using Pandas.\n",
    "\n",
    "    Args:\n",
    "      M : 2D numpy array\n",
    "      rows : list of row labels\n",
    "      cols : list of column labels\n",
    "      dtype : data type (float or int)\n",
    "      float_fmt : format specifier for floats\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(M, index=rows, columns=cols, dtype=dtype)\n",
    "    old_fmt_fn = pd.get_option('float_format')\n",
    "    pd.set_option('float_format', lambda f: float_fmt.format(f))\n",
    "    display(df)\n",
    "    pd.set_option('float_format', old_fmt_fn)  # reset Pandas formatting\n",
    "\n",
    "def pretty_timedelta(fmt=\"%d:%02d:%02d\", since=None, until=None):\n",
    "    \"\"\"Pretty-print a timedelta, using the given format string.\"\"\"\n",
    "    since = since or time.time()\n",
    "    until = until or time.time()\n",
    "    delta_s = until - since\n",
    "    hours, remainder = divmod(delta_s, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "    return fmt % (hours, minutes, seconds)\n",
    "\n",
    "\n",
    "##\n",
    "# Word processing functions\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "    return word\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    word = word.lower()\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset):\n",
    "        return word\n",
    "    else:\n",
    "        return UNK_TOKEN\n",
    "\n",
    "def canonicalize_words(words, **kw):\n",
    "    return [canonicalize_word(word, **kw) for word in words]\n",
    "\n",
    "##\n",
    "# Data loading functions\n",
    "def get_corpus(name=\"brown\"):\n",
    "    import nltk\n",
    "    assert(nltk.download(name))\n",
    "    return nltk.corpus.__getattr__(name)\n",
    "\n",
    "def build_vocab(corpus, V=10000):\n",
    "    import vocabulary\n",
    "    token_feed = (canonicalize_word(w) for w in corpus.words())\n",
    "    vocab = vocabulary.Vocabulary(token_feed, size=V)\n",
    "    return vocab\n",
    "\n",
    "def get_train_test_sents(corpus, split=0.8, shuffle=True):\n",
    "    \"\"\"Generate train/test split for unsupervised tasks.\n",
    "\n",
    "    Args:\n",
    "      corpus: nltk.corpus that supports sents() function\n",
    "      split (double): fraction to use as training set\n",
    "      shuffle (int or bool): seed for shuffle of input data, or False to just\n",
    "      take the training data as the first xx% contiguously.\n",
    "\n",
    "    Returns:\n",
    "      train_sentences, test_sentences ( list(list(string)) ): the train and test\n",
    "      splits\n",
    "    \"\"\"\n",
    "    sentences = np.array(list(corpus.sents()), dtype=object)\n",
    "    fmt = (len(sentences), sum(map(len, sentences)))\n",
    "    print(\"Corpus: {:,} tokens (counting <s>)\".format(len(tokens)))\n",
    "\n",
    "    if shuffle:\n",
    "        rng = np.random.RandomState(shuffle)\n",
    "        rng.shuffle(sentences)  # in-place\n",
    "    train_frac = 0.8\n",
    "    split_idx = int(train_frac * len(sentences))\n",
    "    train_sentences = sentences[:split_idx]\n",
    "    test_sentences = sentences[split_idx:]\n",
    "\n",
    "    fmt = (len(train_sentences), sum(map(len, train_sentences)))\n",
    "    print(\"Training set: {:,} sentences ({:,} tokens)\".format(*fmt))\n",
    "    fmt = (len(test_sentences), sum(map(len, test_sentences)))\n",
    "    print(\"Test set: {:,} sentences ({:,} tokens)\".format(*fmt))\n",
    "\n",
    "    return train_sentences, test_sentences\n",
    "\n",
    "def preprocess_sentences(sentences, vocab, use_eos=False, emit_ids=True):\n",
    "    \"\"\"Preprocess sentences by canonicalizing and mapping to ids.\n",
    "\n",
    "    Args:\n",
    "      sentences ( list(list(string)) ): input sentences\n",
    "      vocab: Vocabulary object, already initialized\n",
    "      use_eos: if true, will add </s> token to end of sentence.\n",
    "      emit_ids: if true, will emit as ids. Otherwise, will be preprocessed\n",
    "          tokens.\n",
    "\n",
    "    Returns:\n",
    "      ids ( array(int) ): flattened array of sentences, including boundary <s>\n",
    "      tokens.\n",
    "    \"\"\"\n",
    "    # Add sentence boundaries, canonicalize, and handle unknowns\n",
    "    word_preproc = lambda w: canonicalize_word(w, wordset=vocab.word_to_id)\n",
    "    ret = []\n",
    "    for s in sentences:\n",
    "        canonical_words = vocab.pad_sentence(map(word_preproc, s),\n",
    "                                             use_eos=use_eos)\n",
    "        ret.extend(vocab.words_to_ids(canonical_words) if emit_ids else\n",
    "                   canonical_words)\n",
    "    if not use_eos:  # add additional <s> to end if needed\n",
    "        ret.append(vocab.START_ID if emit_ids else vocab.START_TOKEN)\n",
    "    return np.array(ret, dtype=(np.int32 if emit_ids else object))\n",
    "\n",
    "\n",
    "def load_corpus(corpus, split=0.8, V=10000, shuffle=0):\n",
    "    \"\"\"Load a named corpus and split train/test along sentences.\n",
    "\n",
    "    This is a convenience wrapper to chain together several functions from this\n",
    "    module, and produce a train/test split suitable for input to most models.\n",
    "\n",
    "    Sentences are preprocessed by canonicalization and converted to ids\n",
    "    according to the constructed vocabulary, and interspersed with <s> tokens\n",
    "    to denote sentence bounaries.\n",
    "\n",
    "    Args:\n",
    "        corpus: (string | corpus reader) If a string, will fetch the\n",
    "            NLTK corpus of that name.\n",
    "        split: (float \\in (0,1]) fraction of examples in train split\n",
    "        V: (int) vocabulary size (including special tokens)\n",
    "        shuffle: (int) if > 0, use as random seed to shuffle sentence prior to\n",
    "            split. Can change this to get different splits.\n",
    "\n",
    "    Returns:\n",
    "        (vocab, train_ids, test_ids)\n",
    "        vocab: vocabulary.Vocabulary object\n",
    "        train_ids: flat (1D) np.array(int) of ids\n",
    "        test_ids: flat (1D) np.array(int) of ids\n",
    "    \"\"\"\n",
    "    if isinstance(corpus, str):\n",
    "        corpus = get_corpus(corpus)\n",
    "    vocab = build_vocab(corpus, V)\n",
    "    train_sentences, test_sentences = get_train_test_sents(corpus, split, shuffle)\n",
    "    train_ids = preprocess_sentences(train_sentences, vocab)\n",
    "    test_ids = preprocess_sentences(test_sentences, vocab)\n",
    "    return vocab, train_ids, test_ids\n",
    "\n",
    "##\n",
    "# Window and batch functions\n",
    "def rnnlm_batch_generator(ids, batch_size, max_time):\n",
    "    \"\"\"Convert ids to data-matrix form for RNN language modeling.\"\"\"\n",
    "    # Clip to multiple of max_time for convenience\n",
    "    clip_len = ((len(ids)-1) / batch_size) * batch_size\n",
    "    input_w = ids[:clip_len]     # current word\n",
    "    target_y = ids[1:clip_len+1]  # next word\n",
    "    # Reshape so we can select columns\n",
    "    input_w = input_w.reshape([batch_size,-1])\n",
    "    target_y = target_y.reshape([batch_size,-1])\n",
    "\n",
    "    # Yield batches\n",
    "    for i in xrange(0, input_w.shape[1], max_time):\n",
    "        yield input_w[:,i:i+max_time], target_y[:,i:i+max_time]\n",
    "\n",
    "\n",
    "def build_windows(ids, N, shuffle=True):\n",
    "    \"\"\"Build window input to the window model.\n",
    "\n",
    "    Takes a sequence of ids, and returns a data matrix where each row\n",
    "    is a window and target for the window model. For N=3:\n",
    "        windows[i] = [w_3, w_2, w_1, w_0]\n",
    "\n",
    "    For language modeling, N is the context size and you can use y = windows[:,-1]\n",
    "    as the target words and x = windows[:,:-1] as the contexts.\n",
    "\n",
    "    For CBOW, N is the window size and you can use y = windows[:,N/2] as the target words\n",
    "    and x = np.hstack([windows[:,:N/2], windows[:,:N/2+1]]) as the contexts.\n",
    "\n",
    "    For skip-gram, you can use x = windows[:,N/2] as the input words and y = windows[:,i]\n",
    "    where i != N/2 as the target words.\n",
    "\n",
    "    Args:\n",
    "      ids: np.array(int32) of input ids\n",
    "      shuffle: if true, will randomly shuffle the rows\n",
    "\n",
    "    Returns:\n",
    "      windows: np.array(int32) of shape [len(ids)-N, N+1]\n",
    "        i.e. each row is a window, of length N+1\n",
    "    \"\"\"\n",
    "    windows = np.zeros((len(ids)-N, N+1), dtype=int)\n",
    "    for i in xrange(N+1):\n",
    "        # First column: first word, etc.\n",
    "        windows[:,i] = ids[i:len(ids)-(N-i)]\n",
    "    if shuffle:\n",
    "        # Shuffle rows\n",
    "        np.random.shuffle(windows)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def batch_generator(data, batch_size):\n",
    "    \"\"\"Generate minibatches from data.\n",
    "\n",
    "    Args:\n",
    "      data: array-like, supporting slicing along first dimension\n",
    "      batch_size: int, batch size\n",
    "\n",
    "    Yields:\n",
    "      minibatches of maximum size batch_size\n",
    "    \"\"\"\n",
    "    for i in xrange(0, len(data), batch_size):\n",
    "        yield data[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-nuZcrMnHKRD"
   },
   "outputs": [],
   "source": [
    "#@title Vocabulary helper functions\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "\n",
    "class Vocabulary(object):\n",
    "\n",
    "  START_TOKEN = u\"<s>\"\n",
    "  END_TOKEN   = u\"</s>\"\n",
    "  UNK_TOKEN   = u\"<unk>\"\n",
    "\n",
    "  def __init__(self, tokens, size=None):\n",
    "    \"\"\"Create a Vocabulary object.\n",
    "\n",
    "    Args:\n",
    "        tokens: iterator( string )\n",
    "        size: None for unlimited, or int > 0 for a fixed-size vocab.\n",
    "              Vocabulary size includes special tokens <s>, </s>, and <unk>\n",
    "    \"\"\"\n",
    "    self.unigram_counts = collections.Counter(tokens)\n",
    "    self.bigram_counts = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    word1 = None\n",
    "    for word in tokens:\n",
    "        if word1 is None:\n",
    "            pass\n",
    "        self.bigram_counts[word1][word] += 1\n",
    "        word1 = word\n",
    "    self.bigram_counts.default_factory = None  # make into a normal dict\n",
    "\n",
    "    # Leave space for \"<s>\", \"</s>\", and \"<unk>\"\n",
    "    top_counts = self.unigram_counts.most_common(None if size is None else (size - 3))\n",
    "    vocab = ([self.START_TOKEN, self.END_TOKEN, self.UNK_TOKEN] +\n",
    "             [w for w,c in top_counts])\n",
    "\n",
    "    # Assign an id to each word, by frequency\n",
    "    self.id_to_word = dict(enumerate(vocab))\n",
    "    self.word_to_id = {v:k for k,v in self.id_to_word.items()}\n",
    "    self.size = len(self.id_to_word)\n",
    "    if size is not None:\n",
    "        assert(self.size <= size)\n",
    "\n",
    "    # For convenience\n",
    "    self.wordset = set(self.word_to_id.keys())\n",
    "\n",
    "    # Store special IDs\n",
    "    self.START_ID = self.word_to_id[self.START_TOKEN]\n",
    "    self.END_ID = self.word_to_id[self.END_TOKEN]\n",
    "    self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
    "\n",
    "  def words_to_ids(self, words):\n",
    "    return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
    "\n",
    "  def ids_to_words(self, ids):\n",
    "    return [self.id_to_word[i] for i in ids]\n",
    "\n",
    "  def pad_sentence(self, words, use_eos=True):\n",
    "    ret = [self.START_TOKEN] + words\n",
    "    if use_eos:\n",
    "      ret.append(self.END_TOKEN)\n",
    "    return ret\n",
    "\n",
    "  def sentence_to_ids(self, words, use_eos=True):\n",
    "    return self.words_to_ids(self.pad_sentence(words, use_eos))\n",
    "\n",
    "  def ordered_words(self):\n",
    "    \"\"\"Return a list of words, ordered by id.\"\"\"\n",
    "    return self.ids_to_words(range(self.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RKOPBVbRHjIO"
   },
   "outputs": [],
   "source": [
    "#@title TSV Corpus Reader\n",
    "import sys, os\n",
    "\n",
    "class TSVCorpusReader(object):\n",
    "    \"\"\"Corpus reader for TSV files.\n",
    "\n",
    "    Input files are assumed to contain one sentence per line, with tokens\n",
    "    separated by tabs:\n",
    "\n",
    "    foo[tab]bar[tab]baz\n",
    "    span[tab]eggs\n",
    "\n",
    "    Would correspond to the two-sentence corpus:\n",
    "        [\"foo\", \"bar\", \"baz\"],\n",
    "        [\"spam\", \"eggs\"]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sentence_file, preload=True, file_reader=open):\n",
    "        \"\"\"Construct a corpus reader for the given file.\n",
    "\n",
    "        Args:\n",
    "            sentence_file: (string) path to a TSV file with one sentence per\n",
    "                line.\n",
    "            preload: (bool) If true, will read entire corpus to memory on\n",
    "                construction. Otherwise, will load on-demand.\n",
    "            file_reader: (function string -> fd) optional replacement for\n",
    "                Python's built-in open(...) method, to be used for reading\n",
    "                from alternative file-like objects.\n",
    "        \"\"\"\n",
    "        self._open = file_reader\n",
    "        self._sentence_file = sentence_file\n",
    "        self._sentence_cache = []\n",
    "\n",
    "        if preload:\n",
    "            self._sentence_cache = list(self.sents())\n",
    "\n",
    "    def _line_iterator(self):\n",
    "        with self._open(self._sentence_file) as fd:\n",
    "            for line in fd:\n",
    "                yield line.strip()\n",
    "\n",
    "    def sents(self):\n",
    "        \"\"\"Iterator over sentences in the corpus.\n",
    "\n",
    "        Yields:\n",
    "            list(string) of tokens\n",
    "        \"\"\"\n",
    "        if self._sentence_cache:\n",
    "            for sentence in self._sentence_cache:\n",
    "                yield sentence\n",
    "        else:\n",
    "            # If no cache, actually read the file.\n",
    "            for line in self._line_iterator():\n",
    "                yield line.split(\"\\t\")\n",
    "\n",
    "    def words(self):\n",
    "        \"\"\"Iterator over words in the corpus.\n",
    "\n",
    "        Yields:\n",
    "            (string) tokens\n",
    "        \"\"\"\n",
    "        for sentence in self.sents():\n",
    "            for word in sentence:\n",
    "                yield word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvSkjdiOHzUk"
   },
   "source": [
    "## Creating the vocabulary\n",
    "\n",
    "Let's now get started with creating the vocabulary. We'll use some of the functions defined in the utility classes we just loaded above.\n",
    "\n",
    "(Note: the following code cell may take 20-30 seconds to complete running.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "4W8Bbx_BENcV",
    "outputId": "5c2bf186-34e6-4e05-dc5b-725ac4a75d17"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create a vocabulary by first canonicalizing all the words -- lowercasing\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# and converting all digits to a single string. The vocabulary maintains a\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# mapping between words and integer ids.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m vocab \u001b[38;5;241m=\u001b[39m Vocabulary(canonicalize_word(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m flatten(\u001b[43mcorpus\u001b[49m))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary: \u001b[39m\u001b[38;5;132;01m{:,}\u001b[39;00m\u001b[38;5;124m words\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(vocab\u001b[38;5;241m.\u001b[39msize))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Turn the corpus into a single flattened list of tokens, where each sentence\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# begins with a special marker <s>.\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a vocabulary by first canonicalizing all the words -- lowercasing\n",
    "# and converting all digits to a single string. The vocabulary maintains a\n",
    "# mapping between words and integer ids.\n",
    "vocab = Vocabulary(canonicalize_word(w) for w in flatten(corpus))\n",
    "print(\"Vocabulary: {:,} words\".format(vocab.size))\n",
    "\n",
    "# Turn the corpus into a single flattened list of tokens, where each sentence\n",
    "# begins with a special marker <s>.\n",
    "tokens = preprocess_sentences(corpus, vocab, use_eos=False, emit_ids=False)\n",
    "print(\"Corpus: {:,} tokens (counting <s>)\".format(len(tokens)))\n",
    "\n",
    "# Retrieve the ids corresponding to the tokens (above). This is the data\n",
    "# we'll actually use.\n",
    "token_ids = vocab.words_to_ids(tokens)\n",
    "print('Sample words:', tokens[:10])\n",
    "print('Sample ids:', token_ids[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixjGRls0YTL4"
   },
   "source": [
    "## The co-occurrence matrix\n",
    "\n",
    "The base for our word embeddings will be a co-occurrence matrix $C$ where $C_{ij}$ is a co-occurrence count for words $i$ and $j$ within some local **window**. This starts to operationalize the idea from Firth about context. For example, consider the text:\n",
    "\n",
    "```\n",
    "the quick brown fox jumped over the lazy dog\n",
    "```\n",
    "\n",
    "With a window of $\\pm 2$ words, we say that `brown`, `fox`, `over`, and `the` form the context of `jumped`.\n",
    "\n",
    "So as we construct $C$, we'd add a count of $1$ for entries (`jumped`, `brown`), (`jumped`, `fox`), (`jumped`, `over`), and (`jumped`, `the`). Since co-occurrence counts are symmetric, we add inverse entries for (`brown`, `jumped`), etc. The diagonal entries of $C$ will be $0$.\n",
    "\n",
    "Note that $C$ has $V^2$ entries, where our vocabulary size $V$ is nearly 50,000. Rather than store nearly 2.5 billion counts, we observe that most of these will be 0. How can we be so sure? Remember that the corpus only contains about 1 million words. With a window size of $\\pm 2$, we only have a total of about 4 million co-occurrences to distribute among those 2.5 billion locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxmWN6FcEQBA"
   },
   "source": [
    "## Constructing a sparse counts matrix\n",
    "\n",
    "We'll define $C$ as a `scipy.sparse` matrix that only stores the nonzero elements.\n",
    "\n",
    "_**Mathematical note:**_  \n",
    "We can compute each element by sliding a window over each position $\\ell$ in the corpus. Suppose our window is size $W = 2K + 1$. Then:\n",
    "\n",
    "$$ C_{ij} = \\sum_\\ell^{|\\text{tokens}|} \\sum_{k \\in [-K,K],\\ k \\ne 0 } \\mathbf{1}[w_\\ell = i \\text{ and } w_{\\ell+k} = j] $$\n",
    "\n",
    "We'll change this a little and modify the order of the sum, which makes for simpler code:\n",
    "\n",
    "$$ C_{ij} = \\sum_{k \\in [-K,K],\\ k \\ne 0 } \\sum_\\ell^{|\\text{tokens}|} \\mathbf{1}[w_\\ell = i \\text{ and } w_{\\ell+k} = j] $$\n",
    "\n",
    "Conveniently, the above is symmetric, so we'll simplify further to:\n",
    "\n",
    "$$ C_{ij}^+ = \\sum_{k = 1}^K \\sum_\\ell^{|\\text{tokens}|} \\mathbf{1}[w_\\ell = i \\text{ and } w_{\\ell+k} = j] = \\sum_{k = 1}^K C_{ij}^+(k)$$\n",
    "\n",
    "$$ C_{ij}^- = \\sum_{k = -K}^1 \\sum_\\ell^{|\\text{tokens}|} \\mathbf{1}[w_\\ell = i \\text{ and } w_{\\ell+k} = j] = \\sum_{k = -K}^1 C_{ij}^-(k)$$\n",
    "\n",
    "It's easy to see that $C_{ij} = C_{ij}^+ + C_{ij}^-$, and since $C_{ij}^+ = C_{ji}^-$, $C$ is a symmetric matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGg29aZQSbZ3"
   },
   "source": [
    "Now we can write the formula in code, where our outer loop sums over $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hPySe-BBEVRy"
   },
   "outputs": [],
   "source": [
    "# A function that produces a sparse co-occurrence matrix given a corpus,\n",
    "# a vocabulary size V, and K (the context window is +-K).\n",
    "def co_occurrence_matrix(token_ids, V, K=2):\n",
    "    # We'll use this as an \"accumulator\" matrix.\n",
    "    C = scipy.sparse.csc_matrix((V,V), dtype=np.float32)\n",
    "\n",
    "    for k in range(1, K+1):\n",
    "        print(u'Counting pairs (i, i \\u00B1 %d) ...' %k)\n",
    "        i = token_ids[:-k]  # current word\n",
    "        j = token_ids[k:]   # k words ahead\n",
    "        data = (np.ones_like(i), (i,j))  # values, indices\n",
    "        Ck_plus = scipy.sparse.coo_matrix(data, shape=C.shape, dtype=np.float32)\n",
    "        Ck_plus = scipy.sparse.csc_matrix(Ck_plus)\n",
    "        Ck_minus = Ck_plus.T  # consider k words behind\n",
    "        C += Ck_plus + Ck_minus\n",
    "\n",
    "    print(\"Co-occurrence matrix: %d words x %d words\" %C.shape) \n",
    "    print(\"  %.02g nonzero elements\" %C.nnz) \n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtdFXP3QEXrU"
   },
   "source": [
    "## Toy corpus example\n",
    "\n",
    "Let's look at a tiny example first (just 2 sentences) to see how this works. With K=1, we should see co-occurrence counts for each pair of neighboring words:\n",
    "\n",
    "`(<s>, nlp)`,  \n",
    "`(nlp, class)`,  \n",
    "`(class, is)`,  \n",
    "\n",
    "and so on, as well as their reversed versions. (Remember, C is symmetric!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "HcdoXTrKEZjg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] <__main__.Vocabulary object at 0x00000256C4EF7250>\n",
      "---------------------------------------------------------------------------\n",
      "Counting pairs (i, i  1) ...\n",
      "Co-occurrence matrix: 8 words x 8 words\n",
      "  0 nonzero elements\n"
     ]
    }
   ],
   "source": [
    "# Build a toy corpus with the same shape as our corpus object.\n",
    "toy_corpus = [\n",
    "    \"nlp class is awesome\",\n",
    "    \"nlp is awesome fun\"\n",
    "]\n",
    "toy_corpus = map(str.split, toy_corpus)\n",
    "\n",
    "# Get vocab, tokens, and token_ids as above.\n",
    "toy_vocab = Vocabulary(canonicalize_word(w)\n",
    "                       for w in flatten(toy_corpus))\n",
    "toy_tokens = preprocess_sentences(toy_corpus, toy_vocab,\n",
    "                                  use_eos=False, emit_ids=False)\n",
    "toy_token_ids = toy_vocab.words_to_ids(toy_tokens)\n",
    "\n",
    "# Build the co-occurrence matrix.\n",
    "\n",
    "print(toy_token_ids, toy_vocab)\n",
    "print('-'*75)\n",
    "toy_C = co_occurrence_matrix(toy_token_ids, toy_vocab.size, K=1)\n",
    "\n",
    "# # Display a table with the counts. The .toarray() function converts the\n",
    "# # sparse matrix into a dense one.\n",
    "# toy_labels = toy_vocab.ordered_words()\n",
    "# pretty_print_matrix(toy_C.toarray(), rows=toy_labels,\n",
    "#                     cols=toy_labels, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(toy_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6G5Ob94EeSh"
   },
   "source": [
    "## Positive pointwise mutual information\n",
    "\n",
    "\n",
    "Before we use the co-occurrence matrix to estimate any word vectors, we're going to make an adjustment to the counts. Rather than use the raw counts, we'll compute the Positive [Pointwise Mutual Information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) (PPMI) for each entry in $C$ instead.\n",
    "\n",
    "PMI is a generalization of the idea of correlation, but for arbitrary variables. Here we're interested in the correlation between word $i$ and word $j$, where we take the samples to be all the word-word pairs in our corpus. Why do we want this instead of raw counts? Well, the raw co-occurrence counts are dominated by common words (e.g. 'the' co-occurs with everything), whereas PMI normalizes co-occurrence counts with the counts of the individual words themselves.\n",
    "\n",
    "The *Positive* part just means we'll truncate PMI values at $0$. Negatives in $C$ can make things difficult when we get to the next step, but intuitively, positive correlations are much more meaningful -- the distinction between $0$ and negative correlation may mostly depend on word frequencies.\n",
    "\n",
    "All the data we need to compute PPMI is already in $C$.\n",
    "\n",
    "First, we compute the pairwise and singleton probabilities ($Z$ is the total number of tokens and $Z_i$ is the total count of token $i$):\n",
    "\n",
    "\\begin{align*}\n",
    "P(i,j) = \\frac{C(i,j)}{\\sum_{k,l} C(k,l)} = \\frac{C_{ij}}{Z} \\\\\\\\\n",
    "P(i) = \\frac{\\sum_{k} C(i,k)}{\\sum_{k,l} C(k,l)} = \\frac{Z_i}{Z} \\\\\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Then compute PMI:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{PMI}(i,j) = \\log \\frac{P(i,j)}{P(i)P(j)} = \\log \\frac{C_{ij} \\cdot Z}{Z_i \\cdot Z_j} \\\\\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Finally, truncate to ignore negatively-correlated pairs:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{PPMI}(i,j) = \\max(0, \\text{PMI}(i,j))\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "#### Note on Sparse Matrices\n",
    "\n",
    "In order to compute PPMI, we'll need to \"unpack\" the nonzero elements. Recall that when we were constructing it, we provided a list of indices:\n",
    "```\n",
    "data = (np.ones_like(i), (i,j))  # values, indices\n",
    "```\n",
    "We'll do the inverse of this here, transform all the values in parallel, and then pack them back into a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pmi(df):\n",
    "    '''\n",
    "    Calculate the positive pointwise mutal information score for each entry\n",
    "    https://en.wikipedia.org/wiki/Pointwise_mutual_information\n",
    "    We use the log( p(y|x)/p(y) ), y being the column, x being the row\n",
    "    '''\n",
    "    # Get numpy array from pandas df\n",
    "    arr = df.as_matrix()\n",
    "\n",
    "    # p(y|x) probability of each t1 overlap within the row\n",
    "    row_totals = arr.sum(axis=1).astype(float)\n",
    "    prob_cols_given_row = (arr.T / row_totals).T\n",
    "\n",
    "    # p(y) probability of each t1 in the total set\n",
    "    col_totals = arr.sum(axis=0).astype(float)\n",
    "    prob_of_cols = col_totals / sum(col_totals)\n",
    "\n",
    "    # PMI: log( p(y|x) / p(y) )\n",
    "    # This is the same data, normalized\n",
    "    ratio = prob_cols_given_row / prob_of_cols\n",
    "    ratio[ratio==0] = 0.00001\n",
    "    _pmi = np.log(ratio)\n",
    "    _pmi[_pmi < 0] = 0\n",
    "\n",
    "    return _pmi\n",
    "\n",
    "\n",
    "\n",
    "pmi(toy_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ps1SDsoYEhua"
   },
   "outputs": [],
   "source": [
    "def PPMI(C):\n",
    "    \"\"\"Tranform a counts matrix to PPMI.\n",
    "\n",
    "    Args:\n",
    "      C: scipy.sparse.csc_matrix of counts C_ij\n",
    "\n",
    "    Returns:\n",
    "      (scipy.sparse.csc_matrix) PPMI(C) as defined above\n",
    "    \"\"\"\n",
    "    # Total count.\n",
    "    Z = float(C.sum())\n",
    "\n",
    "    # Sum each row (along columns).\n",
    "    Zr = np.array(C.sum(axis=1), dtype=np.float64).flatten()\n",
    "\n",
    "    # Get indices of relevant elements.\n",
    "    ii, jj = C.nonzero()  # row, column indices\n",
    "    Cij = np.array(C[ii,jj], dtype=np.float64).flatten()\n",
    "\n",
    "    # PMI equation.\n",
    "    pmi = np.log(Cij * Z / (Zr[ii] * Zr[jj]))\n",
    "\n",
    "    # Truncate to positive only.\n",
    "    ppmi = np.maximum(0, pmi)  # take positive only\n",
    "\n",
    "    # Re-format as sparse matrix.\n",
    "    ret = scipy.sparse.csc_matrix((ppmi, (ii,jj)), shape=C.shape,\n",
    "                                  dtype=np.float64)\n",
    "    ret.eliminate_zeros()  # remove zeros\n",
    "    return ret\n",
    "\n",
    "# Display the PPMI'd version of the co-occurrence matrix.\n",
    "pretty_print_matrix(PPMI(toy_C).toarray(), rows=toy_labels,\n",
    "                    cols=toy_labels, dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2aQ5HVkElGw"
   },
   "source": [
    "## Computing word vectors with SVD\n",
    "\n",
    "We're now ready to do something interesting with our counts matrix.\n",
    "\n",
    "Recall that Singular Value Decomposition (SVD) decomposes a matrix $X$:\n",
    "\n",
    "\\begin{align*}\n",
    "X = UDV^T\n",
    "\\end{align*}\n",
    "\n",
    "where the matrices have the following properties:\n",
    "\n",
    "* $X$ has shape **[m, n]**\n",
    "* $U$ has shape **[m, m]** and is **orthonormal**\n",
    "* $D$ has shape **[m, n]** and is **diagonal** (elements are the ordered **Eigenvalues**)\n",
    "* $V$ has shape **[n, n]** and is **orthonormal**\n",
    "\n",
    "We are not actually interested in perfectly reconstructing our counts matrix. Instead, we want to approximate it with low-dimensional representations that capture as much variance as possible in the original matrix. Since the elements on the diagonal of $D$ are sorted by decreasing Eigenvalue, we keep the first $d$ columns of $U$ as our word vector representations. In our case, $X$ is just our counts matrix $C$, which happens to be symmetric, so $U$ and $V$ are the same.\n",
    "\n",
    "We'll use Sklearn's [TruncatedSVD](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html), which allows us to just compute the first $d$ components (since the full decomposition would be much more costly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Df_kpOsyEnf-"
   },
   "outputs": [],
   "source": [
    "def SVD(X, d=2):\n",
    "    \"\"\"Returns word vectors from SVD.\n",
    "\n",
    "    Args:\n",
    "      X: [m, n] matrix\n",
    "      d: word vector dimension\n",
    "\n",
    "    Returns:\n",
    "      Wv : [m, d] matrix, where each row is a word vector.\n",
    "    \"\"\"\n",
    "    transformer = TruncatedSVD(n_components=d, random_state=0)\n",
    "    Wv = transformer.fit_transform(X)\n",
    "\n",
    "    # Normalize all vectors to unit length.\n",
    "    Wv = Wv / np.linalg.norm(Wv, axis=1).reshape([-1,1])\n",
    "\n",
    "    print 'Computed embeddings:', Wv.shape\n",
    "    return Wv\n",
    "\n",
    "# Compute 3-dimensional word embeddings for the toy corpus.\n",
    "dim = 3\n",
    "embeddings = SVD(PPMI(toy_C).toarray(), d=dim)\n",
    "pretty_print_matrix(embeddings, rows=toy_labels, cols=range(dim), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnruGindZZeo"
   },
   "source": [
    "Now let's try computing word vectors on the large corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwLqHK1LEqFA"
   },
   "outputs": [],
   "source": [
    "# Compute 25-dimensional embeddings with a window of size 2.\n",
    "C = PPMI(co_occurrence_matrix(token_ids, vocab.size, K=2))\n",
    "embeddings = SVD(C, d=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHup870UZe_v"
   },
   "source": [
    "## Visualization\n",
    "\n",
    "For a quick visualization, we can plot the first two dimensions directly.  Plotting two dimensions directly like this is equivalent to just doing the truncated SVD with d=2, which throws away a lot of information -- not much variance of the original matrix is captured with just 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ft7NMsCEZluw"
   },
   "outputs": [],
   "source": [
    "def plot_2d_embeddings(embeddings, n=162):\n",
    "    # Get 1st and 2nd embedding dims for the first n words\n",
    "    x1 = embeddings[:n, 0]\n",
    "    x2 = embeddings[:n, 1]\n",
    "\n",
    "    # Get the corresponding words\n",
    "    word_list = vocab.ids_to_words(range(n))\n",
    "\n",
    "    plt.scatter(x=x1, y=x2)\n",
    "    plt.show()\n",
    "\n",
    "plot_2d_embeddings(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dQ9zfbTErsM"
   },
   "source": [
    "## t-SNE for better visualizations\n",
    "\n",
    "To get a better sense of our embedding structure, we can use [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) instead. This is a *non*-linear way of embedding high-dimensional data (like our embedding vectors) into a low-dimensional space. It works by preserving local distances at the expense of some global distortion. The result is no longer a projection, but because it preserves locality, t-SNE helps us better understand neighborhoods in high dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rtFSOHDvEv9y"
   },
   "outputs": [],
   "source": [
    "tsne_embeddings = tsne(n_components=2).fit_transform(embeddings[:162])\n",
    "plot_2d_embeddings(tsne_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFq-adQRSd5E"
   },
   "source": [
    "## Takeaways\n",
    "\n",
    "We've shown that we can use simple co-occurrences in a text corpus and SVD to create dense embeddings -- representations for words that appear to preserve some of our intuitions about word similarities. A few questions naturally come to mind:\n",
    "\n",
    "* **Is there some way to *interpret* each dimension of the embeddings?**\n",
    "Well, no, not really. Because the embeddings arise solely from their distributional properties, we can't guarantee that each dimension will correspond to some intuitive concept. Of course you could try to examine the 2-D plot (not the t-SNE one) and see if you can find an explanation for the first two components -- the $x$ and $y$ axes. Note that some social science disciplines like psychology have used **Factor Analysis** to attempt to rotate embeddings into a pre-defined space with psychological concepts on the axes. But typically in language processing, we simply relinquish control over the inferred semantics.\n",
    "\n",
    "\n",
    "* **How do we evaluate how *good* the embeddings are?**\n",
    "Evaluation typically comes in two forms: **intrinsic** and **extrinsic**. Intrinsic evaluation attempts to quantify the quality of the embeddings themselves. A few different annotation projects have attempted to score semantic similarity between pairs of words by averaging human judgements (e.g. see [ConceptSim](http://www.seas.upenn.edu/~hansens/conceptSim/)). So we could ask how well distances between our word embeddings correlate with human judgements. Another related evaluation involves analogies. Can we use Euclidean distances in embeddings space to solve fill-in-the-blanks like: `[man::woman as king::_____]`, which can capture various kinds of syntactic and semantic relationships? See [Chen, Peterson, Griffiths, 2017](https://cocosci.berkeley.edu/papers/vector_space_analogy_cogsci2017_final.pdf) for a lot more details and a cognitive science perspective. By contrast, extrinsic evaluation involves plugging the embeddings into another task and measuring relative improvements.\n",
    "\n",
    "As it turns out, the co-occurrence plus SVD method is simple, but inferior with respect to both intrinsic and extrinsic evaluation. The Word2Vec ideas introduced in the next notebook revolutionized language processing by dramatically improving embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2BxDqtDSiJF"
   },
   "source": [
    "## Experiments\n",
    "\n",
    "(1) Compute the 5 nearest neighbors in our induced 25-dimensional embeddings space for each of the following words:\n",
    "`[\"john\", \"kitchen\", \"give\", \"mouse\", \"she\", \"blue\"]`. Do the results look reasonable? Is this intrinsic or extrinsic evaluation?\n",
    "\n",
    "(2) Train your embeddings using a real corpus of text. This could be some text you have access to, e.g. some text from Wikipedia, or Shakespeare. You just need to load a corpus of sentences, which you can do by specifying its pathname as the first argument to the BabiTaskCorpusReader function."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6cbSg2DQPSjj"
   ],
   "name": "Copy of nlu_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:rdevs] *",
   "language": "python",
   "name": "conda-env-rdevs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
